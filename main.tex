%Document Props
%FONT SIZE
%DOC CLASS
\documentclass[12pt, a4paper]{report}

%Packages
\usepackage{fullpage} %1-inch margins
\usepackage{setspace} %double space

% BibLaTeX
%style=ieee looks ugly
\usepackage[backend=bibtex8]{biblatex}
\addbibresource{ref.bib}

%Change Chapter format in report class
\usepackage{titlesec} 
\titleformat{\chapter}[block]
  {\normalfont\huge\bfseries}{\thechapter.}{1em}{\Huge}
\titlespacing*{\chapter}{0pt}{-19pt}{0pt}

%Graphics package and relative path
\usepackage{graphicx}
\graphicspath{{images/}}

% For other font sizes
% !incompatible with fullpage
%\usepackage{extsizes}

% comments
\usepackage{verbatim}

% Hyperlinks package
\usepackage[hidelinks]{hyperref}

% Syntax highlighting package
\usepackage{minted}
\usemintedstyle{minimal}

\onehalfspacing
% ----------------------------------------------------------------------------------------------
\begin{document}

% Outer Cover Page
\pagestyle{empty}
\begin{titlepage}
\vspace*{0.2cm}
\begin{center} \textbf{A REPORT\\ON} \end{center}
\begin{center} \textbf{{\Large AUTOMATIC LAND COVER CLASSIFICATION OF TEMPORAL SATELLITE IMAGES}} \end{center}
\begin{center} \textbf{BY} \end{center}
\begin{center} 
{\Large 
	\begin{tabular}{c c}
	Guntaas Singh & 2018A7PS0269P\\
	Nisarg Vora & 2018A7PS0254P
	\end{tabular}
}
\end{center}
\begin{center} \textbf{AT} \end{center}
\begin{center} \includegraphics{iirs.png} \end{center}
\begin{center} {\Large Indian Institute of Remote Sensing, Dehradun} \end{center}
\begin{center} A Practice School - I station of \end{center}
\begin{center} {\includegraphics{bits.png}} \end{center}
\begin{center} {\Large Birla Institute of Technology and Science, Pilani} \end{center}
\begin{center} June, 2020 \end{center}
\end{titlepage}
\pagebreak

% Inner Cover Page
\begin{titlepage}
\vspace*{0.3cm}
\begin{center} \textbf{A REPORT\\ON} \end{center}
\begin{center} \textbf{{\Large AUTOMATIC LAND COVER CLASSIFICATION OF TEMPORAL SATELLITE IMAGES}} \end{center}
\begin{center} \textbf{BY} \end{center}
\begin{center}
	\begin{tabular}{c c c}
		\textbf{ID Number} & \textbf{Name} & \textbf{Branch} \\
		2018A7PS0269P & Guntaas Singh & B.E. (Hons.) Computer Science \\
		2018A7PS0254P & Nisarg Vora  & B.E. (Hons.) Computer Science \\
	\end{tabular} 
\end{center}
\begin{onehalfspace}
\begin{center} \textbf{Prepared in the partial fulfillment of the} \end{center}
\begin{center} Practice School - I course \end{center}
\begin{center} \textbf{AT} \end{center}
\begin{center} \includegraphics{iirs.png} \end{center}
\begin{center} {\Large Indian Institute of Remote Sensing, Dehradun} \end{center}
\begin{center} A Practice School - I station of \end{center}
\begin{center} {\includegraphics{bits.png}} \end{center}
\begin{center} {\Large Birla Institute of Technology and Science, Pilani} \end{center}
\begin{center} June, 2020 \end{center}
\end{onehalfspace}
\end{titlepage}
\pagebreak

% Acknowledgements
\setcounter{secnumdepth}{0}
\section{Acknowledgments}
\pagestyle{plain}
\pagenumbering{roman}
\setcounter{page}{3}
We would sincerely like to thank the Director of Indian Institute of Remote Sensing, Dr. Prakash Chauhan, for giving us an opportunity to work in this organization and gain exposure to corporate work culture, ethics, and etiquette to be followed while working in a professional environment.
\paragraph{}
We would like to extend our most sincere gratitude to our project in-charge, Dr. Hari Shanker Srivastava, Head of the Programme Planning and Evaluation Group (PPEG) at IIRS, for providing us with the opportunity to work with him on this project, and his guidance and mentorship during the same.
\paragraph{}
We wish to extend our gratitude to the faculty in charge of the PS-I program at IIRS, Rekha A., Assistant Professor at BITS Pilani - Bangalore Center, for her guidance and advice during the PS-I program, and her helpfulness and responsiveness while addressing all the concerns we raised during the same.
\paragraph{}
In addition, we would also like to thank the members of the Practice School Division, for their hard work and dedication in operating the PS-I programme remotely to ensure that we have a seamless learning experience.
\begin{flushright}
Guntaas Singh\\
Nisarg Vora
\end{flushright}
\pagebreak

% Details and abstract
\begin{center}  
\textbf {BIRLA INSTITUTE OF SCIENCE AND TECHNOLOGY\\
PILANI (RAJASTHAN)\\
Practice School Division}
\end{center}
\begin{onehalfspace}
\textbf{Station:} Indian Institute of Remote Sensing \\
\textbf{Centre:} Dehradun\\
\textbf{Duration:} 3 weeks\\
\textbf{Date of start:} 18th May, 2020 \\
\textbf{Date of submission:} 6th June, 2020 \\
\textbf{Title of project:} Automatic Land Cover Classification of Temporal Satellite Images
\begin{center}
\begin{tabular}{c c c}
\textbf{ID Number} & \textbf{Name} & \textbf{Branch} \\
2018A7PS0269P & Guntaas Singh & B.E. (Hons.) Computer Science \\
2018A7PS0254P & Nisarg Vora  & B.E. (Hons.) Computer Science \\
\end{tabular} 
\end{center}
\textbf{Name of guide:} Dr. Hari Shanker Srivastava \\
\textbf{Designation:} Scientist/Engineer - SG. Group Head, Programme Planning and Evaluation Group (PPEG). \\
\textbf{Name of PS faculty:} Rekha A. \\
\textbf{Key Words:} Semantic Segmentation, Support Vector Machines, Convolutional Neural Networks, U-Net Architecture\\
\textbf{Project Areas:} Remote Sensing, Machine Learning, Deep Learning
\section{Abstract}
\paragraph{}
This project attempts automatic land cover classification of different parts of India into forest, urban, agricultural land and water bodies which can be used for various applications like natural resource management, urban expansion etc.  Data from Agra district, Uttar Pradesh has been used to train Support Vector Machines and Convolutional Neural Networks which are then tested on Gandhinagar district, Gujarat. Google Earth Engine has been used to obtain data from Landsat 8 satellite images. For the purpose of classification Normalized Difference Vegetation Index (NDVI) values are calculated by masking all other light bands except near-infrared and red light bands. Temporal images with NDVI labels are fed as input to the models and subsequently, the accuracy of these models is reported.
\end{onehalfspace}
\newpage

%Response sheet
\begin{comment}
	\begin{center}  
	\textbf {BIRLA INSTITUTE OF SCIENCE AND TECHNOLOGY\\
	PILANI (RAJASTHAN)\\
	Practice School Division\\
	Response Option Sheet}
	\end{center}
	\begin{onehalfspace}
	\textbf{Station:} Indian Institute of Remote Sensing \\
	\textbf{Centre:} Dehradun
	\begin{center}
	\begin{tabular}{c c c}
	\textbf{ID Number} & \textbf{Name} & \textbf{Branch} \\
	2018A7PS0269P & Guntaas Singh & B.E. (Hons.) Computer Science \\
	2018A7PS0254P & Nisarg Vora  & B.E. (Hons.) Computer Science \\
	\end{tabular} 
	\end{center}
	\textbf{Title of project:} Automatic Land Cover Classification of Temporal Satellite Images\\''
	\vspace*{1cm}
	Usefulness of the project to the on-campus courses of study in various disciplines. Project should be scrutinized keeping in view the following response options. Write Course No. and Course Name against the option under which the project comes.\\
	\begin{center}
	\begin{tabular}{|p{1cm}|p{10cm}|p{3cm}|}
	\hline
	\textbf{Code No.} & \textbf{Response Option} & \textbf{Course No.(s) and Name} \\
	\hline
	1 & A new course can be designed out of this project. & ~\\
	\hline
	2 & The project can help modification of the course content of some of the existing Courses & ~\\
	\hline
	3 & The project can be used directly in some of the existing Compulsory Discipline Courses (CDC)/ Discipline Courses Other than Compulsory (DCOC)/ Emerging Area (EA), etc. Courses & ~\\
	\hline
	4 & The project can be used in preparatory courses like Analysis and Application Oriented Courses (AAOC)/ Engineering Science (ES)/ Technical Art (TA) and Core Courses. & ~\\
	\hline
	5 & This project cannot come under any of the above mentioned options as it relates to the professional work of the host organization. &~ \\
	\hline
	\end{tabular} 
	\end{center}
	\end{onehalfspace}
	\vspace*{0.5cm}
	\textbf{Signature}\\
	\textbf{Date: }
	\newpage
\end{comment}

% Table of Contents
\tableofcontents
\listoffigures

% INTRODUCTION
\setcounter{page}{1}
\setcounter{secnumdepth}{2}
\chapter{Introduction}
\pagenumbering{arabic}
\section{About IIRS}
Formerly known as Indian Photo-interpretation Institute (IPI), the institute was founded on 21st April, 1966 under the aegis of Survey of India (SOI). It was established in collaboration with the Government of The Netherlands on the pattern of Faculty of Geo-Information Science and Earth Observation (ITC) of the University of Twente, The Netherlands. The original idea of setting up the institute came from India's first Prime Minister - Pandit Jawahar Lal Nehru during his visit to The Netherlands in 1957. 
\paragraph{}
Since its establishment in 1966, IIRS is a key player for training and capacity building in geo-spatial technology and its applications through training, education and research in Southeast Asia. The training, education and capacity building programmes of the institute are designed to meet the requirements of professionals at working levels, fresh graduates, researchers, academia, and decision makers.
\cite{iirs.about.history, iirs.about.instiprof}
\section{Remote Sensing}
Remote sensing is the method of getting information about Earth and its surface without making any physical contact, primarily using satellites. Remote sensing is used in numerous fields, including geography, land surveying and most Earth science disciplines (for example, hydrology, ecology, meteorology, oceanography, glaciology, geology); it also has military, intelligence, commercial, economic, planning, and humanitarian applications.\cite{remotesensingwiki}
It may be split into "active" remote sensing and "passive" remote sensing. In active remote sensing method, sensors are used to capture the reflection of the signals emitted by the satellite on different objects. In passive remote sensing, the reflection captured by the sensors is mainly due to sunlight. In this project, we are using passive remote sensing data collected by Landsat 8 satellite.

\section{Objective: Land Cover Classification}
Land Cover classification generally refers to the categorization or classification of human activities and natural elements on the landscape within a specific time frame based on established scientific and statistical methods of analysis of appropriate source materials. Land cover refers to the surface cover on the ground like vegetation, urban infrastructure, water, bare soil etc. Identification of land cover establishes the baseline information for activities like thematic mapping and change detection analysis.\cite{landcoversig}
\subsection{Techniques}
Mainly two techniques are used for land cover classification: Field survey and Remotely sensed imagery analysis. Field survey was the primary method used for land cover classification until a few decades back. However, it was very inaccurate and time-consuming. With developments in technology, the conventional method of field survey which can be inaccurate and time consuming has been replaced by remotely sensed imagery analysis which is highly accurate and covers wider range.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{landcover.png}
\caption{NDVI distribution in 2000 and 2015 in Kathmandu valley, Nepal, derived from MODIS data. Depicts changes in land cover/land use over time.}
\cite{lcimg}
\end{figure}
\subsection{Significance}
Land cover classification data is of great importance in the following applications:
\begin{itemize}
\item \textbf{Natural Resource Management:} Land cover classification can help policy makers decide how to best manage all the available resources by looking at different terrains. For example, the government can decide which area requires irrigation facilities the most using information about the land cover class distribution. 
\item \textbf{Wildlife habitat protection:} With shrinking habitats of wild animals, land cover classification can provide important data for helping in wildlife preservation and protection.
 \item \textbf{Urban Expansion/encroachment:} Proper planning is required for expansion of urban area. Different land cover features need to be studied to minimize the time and economic cost required for tackling natural obstacles to urban expansion.
 \item \textbf{Damage delineation (Tornadoes, flooding, fire, volcanic):} Live land cover data can provide us with information about where a certain natural disaster (like flood) has occurred/can occur. This can also help us to identify the areas at maximum risk of damage.
 \item \textbf{Target Detection - identification of land strips for use:} Using land cover data, identification of ideal locations for different urban construction becomes easy. For example, one can decide where exactly it is possible to build an airport, bridge etc.
\end{itemize}

\newpage

% ____________________________

% BACKGROUND	
\chapter{Background}

\section{Normalized Difference Vegetation Index(NDVI)}
Satellite data has become a  very important parameter for judging crop progress, land cover classification etc. However, satellite data can not directly report crop health or classify land cover into different classes. NDVI which is calculated by measuring the reflectance of near-infrared and red light is probably the most important and accurate parameter for farmers to know crop health. NDVI values are so accurate that they can in fact, inform farmers about an upcoming drought upto two weeks in advance. Farmers can then make corresponding decisions to increase irrigation and other crop supplements.
\subsection{What is NDVI?}
Light from the sun falls on Earth in three major light bands: ultraviolet, infrared and red. Most of the ultraviolet light is reflected back by the atmosphere. Healthy green plants absorb most of the visible light for photosynthesis. However, to prevent over-heating and dehydration the light in red band and near-infrared band is reflected back by the plants. The red light is visible to humans, while infrared is not. The NDVI values are calculated using the reflectance in red and near-infrared regions.
\subsection{Formula}
NDVI is calculated in accordance with the formula:
\begin{displaymath}
NDVI=\frac{NIR-RED}{NIR+RED}
\end{displaymath}
where,\\
$NIR=$ reflection in the near-infrared spectrum\\
$RED=$ reflection in the red range of the spectrum.
\paragraph{}
As per the formula above, the density of vegetation (NDVI) at a certain point of the image is equal to the difference in the intensities of reflected light in the red and infrared range divided by the sum of these intensities.
\paragraph{}
NDVI values range from -1.0 to 1.0, the positive values are usually for plants and vegetation where as the negative values usually indicate presence of snow, water or cloud cover. Values very close to 0 and less than 0.1 are due to presence of urban built ups rocks etc. Moderate values between 0.2 and 0.3 are due to shrubs and crops whereas large values close to 1.0 represent temperate and tropical forests. These rough trends indicate the relevance of NDVI to the problem of land cover classification.
\begin{figure}[ht!]
\includegraphics[width=.45\textwidth]{ndvisampleone.jpg}\hfill
\includegraphics[width=.45\textwidth]{ndvisampletwo.jpg}
\caption{(1) Without NDVI. (2) With NDVI.}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{ndviillustration.jpg}
\caption{Plant health directly affects reflectance in NIR and RED bands.}
\end{figure}
\subsection{Correlation with plant health}
The amount of chlorophyll acts as a health indicator for the plants. Chlorophyll strongly absorbs visible light and the cell structure reflects near-infrared light. When chlorophyll levels are low, the cell structure gets destroyed and starts absorbing near-infrared light instead of reflecting it. Thus, value of NIR without vegetation decreases and hence the NDVI value becomes smaller.

\section{Image Classification}
Image classification is a standard task in computer vision. In general, the image classification problem involves assigning one  label out of a given fixed set of discrete labels to the input image on the basis of its visual content. While this is a trivial task for humans, robust image classification is a big challenge for a machine. To the computer, the image is just a grid of numbers which entirely change in unreliable ways with variations in viewpoint, illumination, occlusion, etc. As a result, there is no obvious algorithm which solves this problem. However, a data driven approach of providing the machine with many examples of each class and use of machine learning techniques has shown to be useful.\cite{cs231n}
\paragraph{}
There are different ways in which these techniques can be applied for classification of satellite imagery.
\subsection{Pixel Based Approach}
In typical satellite images, pixel sizes are generally similar in size to the objects of interest. Most of the methods for image analysis using remote sensing data work on a per-pixel basis. However, with advances in remote sensing technology, the spatial resolution has become finer than the typical objects of interest, leading to an increase in within-class variability.\cite{eyesky}
\subsection{Object Based Approach}
The term "objects" represents meaningful semantic entities or scene components that are distinguishable in an image.\cite{eyesky} This approach involves the partition of the image into meaningful geographical objects that share relatively homogeneous color, texture, etc.
\subsection{Semantic Approach}
This aims to label each scene image with a specific semantic class. Here, a scene image refers to a local image patch manually extracted from large scale remote sensing images that contain explicit semantic classes.\cite{eyesky}

\newpage
\section{Deep Learning and Neural Networks}
Application of traditional machine learning techniques requires handcrafted features, developing which demands a considerable amount of engineering skill and domain expertise. This, however, is not true for neural networks, which automatically learn these features from data using a general-purpose learning procedure.\cite{eyesky, cs231n} 
%Despite having been around for decades, neural networks have garnered much attention only in the last few years on account of the availability of increased computational power and large amounts of data.
\paragraph{}
A standard neural network consists of many simple, connected processors called neurons, each producing a sequence of real-valued activations. Input neurons get activated through data applied at the input of he network. Other neurons get activated through weighted connections from previously active neurons. \cite{schmidhuber2015deep} Each neuron can be seen as a single unit applying a non-linear activation function (such as sigmoid, tanh, ReLU) to a linear combination of the input activations to the neuron.\cite{cs229}
\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{nn1.png}
\caption{A single neuron.}\cite{cs229}
\end{figure}
\paragraph{}
These single neurons can be stacked so that one neuron passes its output as input into the next neuron. The resulting network of neurons can, hence, consist of several layers of neurons, each with their own learnable weights and biases. Used in conjunction with an appropriate loss function and optimization algorithm, such a network can be used to learn any complex function, if sufficient data is available for training. Forward propagation through the network yields its prediction for a given input. This prediction is compared with the actual class label, and the loss is computed. Backward Propagation is used to compute the gradients of the loss function with respect to the parameters, which are then used by the optimization algorithm to adjust the parameters and minimize the loss over a number of iterations.
\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{nn2.png}
\caption{A two layer neural network with fully connected layers.}\cite{cs231n}
\end{figure}

\section{Convolutional Neural Networks}
Regular neural networks do not scale well to full images. If the input to the neural network is a 200x200 RGB image, the number of weights for each neuron will be 200*200*3 = 120,000 weights. For large networks, the total number of learnable parameters become very large and lead the model to potentially overfit the training data, unless the training set is adequately large.
\paragraph{}
A convolutional neural network (CNN) is a sequence of layers. Each layers transforms an input volume (images are represented as a three dimensional matrix) of activations to another with some differentiable function which may or may not have parameters.\cite{cs231n, dlai4} These layers are of three main types:
\subsection{Convolutional Layer}
This is the core building block for convolutional networks. It is based on the convolution operation on images.
\paragraph{}
Each convolutional layer of a CNN consists of $N$ kernels or filters of a certain volume of neurons sized $f \times f \times d$, with $f$ being the spatial dimension and $d$ being the number of feature channels of the kernel, which is same as the number of channels in the image at its input ($D_{i}$). Every one of these filters is slid through the entire image of size $H_{i} \times W_{i} \times D_{i}$. Convolution refers to the summation of the element-wise dot product of the neurons in each filter with the corresponding values in the input, for each position in which the filter is aligned with the image. Based on this notion, a convolution with a single filter at each layer results in a two dimensional output of a certain size. \cite{cs231n, muruganandham2016semantic, dlai4}
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{cnn1.jpg}
\caption{The convolution operation performed using a 3x3 filter on a 5x5x1 image.}\cite{fathi2018deep}
\end{figure}
\paragraph{}
The intervals with which the filter moves in each spatial dimension is decided by the \textbf{stride} $s$. In order to prevent undue shrinkage of the volume along the spatial dimension, the image can be padded with pixels along the outer edges. The width of this \textbf{padding}, in pixels, is given by another hyper-parameter, $p$.\cite{dlai4, muruganandham2016semantic} The convolution operation is repeated for each of the $N$ filters, and the resulting $N$ activation maps are stacked together across the third dimension giving an output volume of dimensions:\\
\begin{displaymath}
H_{o}=\frac{H_{i}-f+2p}{s}+1
\end{displaymath}
\begin{displaymath}
W_{o}=\frac{W_{i}-f+2p}{s}+1
\end{displaymath}
\begin{displaymath}
D_{o}=N
\end{displaymath}
\paragraph{}
In a convolutional layer, each neuron is connected to only a local region of the input volume, called the receptive field of that neuron. The extent of connectivity is limited to the filter size along the spatial dimensions, but is full along the depth axis.\cite{cs231n} It should also be noted that all activations belonging to a particular channel in the output volume correspond to a single filter applied on the input volume, and hence depend on the same shared parameters. Local connectivity and parameter sharing not only help reduce the number of learnable parameters, but also make the CNN good at capturing \textbf{translation invariance}. This makes them an ideal choice for the image classification problem.
\subsection{Pooling Layer}
It is common to periodically insert a Pooling layer in-between successive convolutional layers in a CNN. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting.\cite{cs231n} The most common form of pooling layer in CNN architectures employs filters of size $2 \times 2$ with a stride of 2, taking a max over 4 cells of the input image. It is worth noting that while this halves the width and height of the image, the depth remains unaffected as the max operation is applied independently to each channel of the image. This down-sampling process effectively discards 75\% of the activations.
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{cnn2.jpg}
\caption{(1) A typical max pooling layer. (2) The max pooling operation.}\cite{cs231n}
\end{figure}
\subsection{Fully Connected Layer}
Once higher level features are detected from the preceding convolution and pooling layers, a fully connected layer is usually attached at the end of the network. This layer is fully connected to all activations in the previous layer, as in regular neural networks, allowing all the features learned by the network to be taken into account by the output layer.
\paragraph{}
All of these different types of layers can be stacked together in various ways to form a CNN.
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{cnn3.jpg}
\caption{AlexNet - an influential NN architecture that popularized the use of CNNs and GPUs to accelerate deep learning.}\cite{alexnet, alexnetimg}
\end{figure}

\section{Semantic Segmentation}
Typical CNNs used for image classification take fixed-size inputs and produce non-spatial outputs (predicted class labels). However, in case of land cover classification, the objective is not to assign a single class label to a given satellite image, but to divide the given image into segments each corresponding to one class out of a given set of land cover classes. This problem is known as semantic segmentation. There are different ways in which the existing image classification CNNs can be extended to solve this problem.
\paragraph{}
One approach is to use networks with only convolutional layers. In these networks, zero padding can be used to preserve the spatial dimensions of the input image. However, the large spatial dimensions throughout the network make the model computationally infeasible. \cite{long2015fully, cs231n, unet}
\paragraph{}
While the problem requires the CNN to produce a spatial output, down-sampling is essential for reducing the need for computational resources to a feasible level. As a result, the most popular approach is to design the network as a combination of an encoder(down-sampling path) and a decoder(up-sampling path).\\

\section{FCN Architecture}
Fully connected layers have an equivalent representation as a convolutional layer having $N$ filters with dimensions equal to those of the input image. The output of this layer will thus be a volume of dimensions $1 \times 1 \times N$. This simple change allows the same CNN to be applied on images with arbitrary spatial dimensions and classify them in a single pass of forward propagation. This is far more efficient than iterating on different crops and classifying one pixel at a time. This is the basic intuition behind \textbf{Fully Convolutional Networks}.\cite{long2015fully} For up-sampling the classification maps produced by a FCN, \textit{Long et al.}\cite{long2015fully} propose the use of transpose convolution. 
\subsection{Transpose Convolution}
Ordinary convolution involves taking a dot product between the filter and the input for every receptive field at a stride of s in the input, and storing the result at a stride of 1 in the output. The resulting output image gets down-sampled by a factor of s. In transpose convolution, we use the values at a stride of 1 in the input and take their scalar product with the filter, storing the resulting matrix at a stride of s in the output. As a result the image gets up-sampled by a factor of s. Any overlapping values in the output are summed together.\cite{long2015fully, cs231n}
A stack of layers performing transpose convolution with an appropriate activation function can be used to learn a non-linear up-sampling.\cite{long2015fully}\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{fcn.jpg}
\caption{A $3 \times 3$ transpose convolution with stride 2 and padding 1. The output gets up-sampled by a factor of 2.}
\cite{cs231n}
\end{figure}
\subsection{Skip Connections}
Semantic segmentation faces an inherent tension between semantics and location. Deep layers capture local information and resolve the semantic information that the image contains. Global information available to shallower layers can be used to capture the location. This motivates the idea of adding skip connections from shallow layers to the final prediction layer. Combining the information captured by shallow and deep layers at the time of up-sampling lets the model make local predictions that take into account global structure and semantics. To combine activations with different spatial dimensions, they are first up-sampled individually as required using transpose convolutions and then summed up. The result is then up-sampled by a final layer, so that the output spatial dimensions match those of the input image. The addition of skip connections produces a finer and more detailed segmentation map.\cite{long2015fully}
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{fcn2.jpg}
\caption{The different variants of the FCN architecture defined in \cite{long2015fully}, with different skip connections added to the base network.}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{fcn3.jpg}
\caption{Different networks based on the FCN architecture.}
\cite{long2015fully}
\end{figure}
% ----------------------------------

% METHODOLOGY & IMPLEMENTATION
\chapter{Methodology and Implementation}

\section{Software Overview}
\subsection{Google Earth Engine}
Google Earth Engine is a computing platform that allows users to run geo-spatial analysis on Google's cloud infrastructure. It provides several ways to interact with the platform. The Code Editor is a web-based Integrated Development Environment (IDE) for writing and running scripts. The Explorer is a lightweight web app for exploring the data catalog and running simple analyses. The client libraries provide Python and JavaScript wrappers around the web Application Programming Interface (API).\cite{gee1} In our project, the \textbf{JavaScript API} was used to visualize, prepare, process, and export satellite imagery and data.
\subsection{Google Colab}
Google Colab is a hosted Jupyter notebook service that provides a web-based development environment for writing and executing arbitrary Python code within a browser. It is particularly well suited for machine learning and data analysis applications. It requires no setup to use, enables quick and easy collaboration and provides free access to computing resources like GPUs.\cite{colab} These computational resources are of great importance for training any classifier as they can significantly reduce training time. Since applying learning techniques is a very empirical process, the ability to iterate quickly is indispensable.
\subsection{Python}
Python is an interpreted, high-level, general purpose programming language. It has an easy to learn syntax that emphasizes code readability. It supports multiple programming paradigms and has a comprehensive standard library. It also has a well-developed ecosystem of libraries for scientific computing and data science such as \textbf{NumPy}, \textbf{Pandas} and \textbf{Matplotlib}. These libraries provide data structures and methods for handling large amount of data efficiently and for high-performance computation. In addition to these libraries, in our project, we also used \textbf{Pillow} (Python Imaging Library) for image processing and \textbf{GDAL} (Geospatial Data Abstraction Library) for handling images in the \textbf{GeoTIFF} file format.
\subsection{Scikit-learn and Scikit-image}
Scikit-learn is a free software machine learning library for Python. It features various algorithms for supervised and unsupervised learning tasks such as classification, regression and clustering. It heavily depends on the NumPy library for high-performance linear algebra. In our project, it was used for implementing various models based on machine learning algorithms such as support vector machines, k-nearest neighbours and decision tree classification.  for data pre-processing and augmentation. The Scikit-image library was used for augmentation of training data.\cite{scikit}
\subsection{TensorFlow}
TensorFlow is an end-to-end open source platform for deep learning. It provides high-level APIs based on the Keras API standard which enable quick and easy implementation of deep neural networks. It also provides a library of different optimizers, losses and metrics for training the network. Modules for handling and pre-processing different types of data are also available. It can also be used to implement complex networks with non-linear topology. We implemented such a network based on the U-Net Architecture using the \textbf{Keras Functional API}.\cite{tensorflow}
\section{Data Collection}
\subsection{Datasets}
\subsubsection{Landsat 8}
Time series data for NDVI was collected from the \textit{Landsat 8 Collection 1 Tier 1 8-Day NDVI Composite} product. The data is collected by Landsat 8 - an American Earth Observation satellite launched in 2013 under Landsat, a joint program of the United States Geological Survey (USGS) and National Aeronautics and Space Administration (NASA). The satellite images the entire Earth's surface at a spatial resolution of 30m, once every two weeks and records multi-spectral and thermal data. The Operational Land Imager (OLI) collects passive remote sensing data from nine spectral bands including the Near Infrared and Visible bands which we intend to use for the project. The data has been atmospherically corrected in addition to standard geometric adjustments and geo-referencing. Information from the thermal infrared sensor is also used to mask clouds. The imagery, hence, provides a clear view of land. The high temporal resolution of 8 days makes the dataset suitable for the purpose of our project.\cite{geel8, l8}
\subsubsection{Copernicus Global Land Cover dataset}
The \textit{Copernicus Global Land Service (CGLS) - LC100 collection 2} is earmarked as a component of the Land service to operate a multi-purpose service component that provides a series of bio-geophysical products on the status and evolution of land surface at global scale. Derived from the PROBA-V 100m time series data for the year of 2015, this dynamic land cover map at 100 m spatial resolution provides a primary land cover scheme. Next to these discrete classes, the product also includes continuous field layers for all basic land cover classes that provide proportional estimates for vegetation/ground cover for the land cover types.\cite{geecglc}
\subsection{Study Areas}
For training our classifiers, we used images of Agra, Mathura and the surrounding region, Uttar Pradesh. This location was chosen as it has a relatively even distribution of different land cover classes. Additionally, first-hand knowledge of the land cover of the region was provided by the mentor which helped us refine the data. For testing, images of Ahmedabad and Gandhinagar, Gujarat were used because of similar reasons.
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{agra.jpg}
\caption{Different views of the training location - Agra. (1) True colour satellite image (2) False colour infrared image (3) Map of NDVI values (4) Ground truth land cover. [Google Earth Engine]}
\end{figure}
\newpage
\subsection{Data Preparation}
All the data required was collected and prepared using Google Earth Engine Javascript API.
\subsubsection{NDVI time series data}
The Landsat 8 NDVI Composite product was first imported as an ImageCollection (a class defined in the API, representing a set of images). The extents of the images were constrained to the required region - Agra and Mathura. The collection was then filtered for bimonthly periods (January to February, March to April, and so on). While a higher temporal resolution is desirable, it would not be possible to mask all cloudy regions using the limited number of images in a smaller duration. After filtering, the median NDVI value was computed for every pixel in each bimonthly period. The six images thus achieved were stacked together, producing a single six-band image.
\subsubsection{Ground truth land cover data}
The Copernicus Global Land Cover dataset has different "coverfraction" bands which provide proportional estimates for different land cover classes classes. Appropriate thresholds were estimated for each of these bands through an iterative process of comparison with land cover maps available via ISRO's Bhuvan platform. The masks obtained by applying these thresholds were concatenated to arrive at a mutually exclusive and exhaustive classification of the study area. The maps obtained were found to be have satisfactory accuracy but suffered from the low spatial resolution of the original dataset - 100m. The project aims to produce finer land cover maps by operating on satellite imagery with a spatial resolution of 30m.
The land cover classes used are:
\begin{enumerate}
\item Built-up land, including urban and rural areas
\item Cultivated and managed vegetation
\item Water bodies
\item Forest land
\end{enumerate}
\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{lcbands.jpg}
\caption{Masks for different land cover classes for Agra and Mathura region.}
\end{figure}
\section{Data Preprocessing}
\section{Linear Discriminant Analysis}
\paragraph{} Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition, and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events.
LDA is also closely related to principal component analysis (PCA) and factor analysis in that they both look for linear combinations of variables which best explain the data. LDA explicitly attempts to model the difference between the classes of data. PCA, in contrast, does not take into account any difference in class, and factor analysis builds the feature combinations based on differences rather than similarities. Discriminant analysis is also different from factor analysis in that it is not an interdependence technique: a distinction between independent variables and dependent variables (also called criterion variables) must be made.

Consider a set of observations ${\vec {x}}$ (also called features, attributes, variables or measurements) for each sample of an object or event with known class $y$. This set of samples is called the training set. The classification problem is then to find a good predictor for the class$y$ of any sample of the same distribution (not necessarily from the training set) given only an observation ${\vec {x}}$.

LDA instead makes the additional simplifying homoscedasticity assumption (i.e. that the class covariances are identical, so  $(\Sigma _{0}=\Sigma _{1}=\Sigma )$ and that the covariances have full rank. In this case, several terms cancel:

${\vec {x}}^{T}\Sigma _{0}^{-1}{\vec {x}}={\vec {x}}^{T}\Sigma _{1}^{-1}{\vec {x}}$
${\displaystyle {\vec {x}}^{T}{\Sigma _{i}}^{-1}{\vec {\mu }}_{i}={{\vec {\mu }}_{i}}^{T}{\Sigma _{i}}^{-1}{\vec {x}}}{\displaystyle {\vec {x}}^{T}{\Sigma _{i}}^{-1}{\vec {\mu }}_{i}={{\vec {\mu }}_{i}}^{T}{\Sigma _{i}}^{-1}{\vec {x}}}$ because ${\displaystyle \Sigma _{i}}\Sigma _{i}$ is Hermitian
and the above decision criterion becomes a threshold on the dot product

${\displaystyle {\vec {w}}\cdot {\vec {x}}>c}{\vec {w}}\cdot {\vec {x}}>c$
for some threshold constant c, where

${\displaystyle {\vec {w}}=\Sigma ^{-1}({\vec {\mu }}_{1}-{\vec {\mu }}_{0})}{\vec {w}}=\Sigma ^{-1}({\vec {\mu }}_{1}-{\vec {\mu }}_{0})$
${\displaystyle c={\vec {w}}\cdot {\frac {1}{2}}({\vec {\mu }}_{1}+{\vec {\mu }}_{0})}{\displaystyle c={\vec {w}}\cdot {\frac {1}{2}}({\vec {\mu }}_{1}+{\vec {\mu }}_{0})}$
This means that the criterion of an input ${\displaystyle {\vec {x}}}{\displaystyle {\vec {x}}}$ being in a class ${\displaystyle y}y$ is purely a function of this linear combination of the known observations.

It is often useful to see this conclusion in geometrical terms: the criterion of an input$ {\displaystyle {\vec {x}}}{\displaystyle {\vec {x}}}$ being in a class ${\displaystyle y}y$ is purely a function of projection of multidimensional-space point$ {\displaystyle {\vec {x}}}{\displaystyle {\vec {x}}}$ onto vector ${\displaystyle {\vec {w}}}{\displaystyle {\vec {w}}}$ (thus, we only consider its direction). In other words, the observation belongs to ${\displaystyle y}y$ if corresponding ${\displaystyle {\vec {x}}}{\displaystyle {\vec {x}}}$ is located on a certain side of a hyperplane perpendicular to ${\displaystyle {\vec {w}}}{\displaystyle {\vec {w}}}$. The location of the plane is defined by the threshold c.

\subsection{Project Specifics}
\paragraph{}
 During the sowing season the agricultural land is mostly barren. Therefore, the classifier may tend to classify it as wasteland if the classification is made solely based on NDVI values. Similarly, in dry seasons, dry rivers might get classified as barren land too. To prevent this, we use a data preprocessing method- Linear Discriminant Analysis.
\paragraph{}
Linear discriminant analysis also worked as a dimensionality reduction technique for this project which drastically reduced the processing time and made results more accurate. The accuracy of the results increased from 78 per cent without LDA to almost 97 per cent after preprocessing with LDA method. Since NDVI values were calculated bi-monthly, the data initially had 6 features i.e. 6 dimensional. As the data points were classified into 4 different classes, LDA reduced the number of features to 3 i.e. 3 dimensional.
\subsection{Patch generation}
Since, the neural network we implemented expects inputs to have dimensions of $256 \times 256$, the satellite images extracted cannot be used directly. To this end, a patch generation system was implemented. The system takes images of any width and height as input and produces cropped image patches of the required dimensions using a given stride. The patches obtained cover the input image completely, irrespective of its dimensions. A stride smaller than the size can also be used to produce overlapping patches. A reconstruction system was also implemented to restitch the patches produces as output by the neural network, with a provision for handling overlapping patches.
\subsection{Data Augmentation}
Deep neural networks typically require a large amount of training data. It may not be possible for the network to perform well unless the training data provides a representative sample of the distribution of different types of land cover in different areas. Therefore, to augment the size of the training dataset, every patch was rotated by $90\circ$ thrice. This allowed us to train the network on four times as much data. It is expected to have helped the network to deal with spatial variations better. Since, machine learning algorithms do not require as much data and consume flattened data in a tabular format, this augmentation was not done for those models.
\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{dataaug.jpg}
\caption{Patch generation and data augmentation.}
\end{figure}
\section{Machine Learning Techniques}
The project aims to achieve automatic land cover classification. To do so, it becomes extremely important to compare accuracy of different models and  consider the best out of them.
\subsection{Support Vector Machines}
\subsection{Overview}
Supervised learning is the machine learning task of learning a function that maps an input to an output on the basis of training data.\cite{supervisedlearningone, supervisedlearningtwo} The training data is the set of input-output example pairs used to train the classifier. If enough data is available then the trained model usually identifies the underlying data patterns and is then able to make predictions on previously unknown input. Supervised learning, however, faces the problem of overfitting when model is made very flexible in order to fit the underlying data and underfitting when the model is to rigid to fit the underlying data.
\subsection{Support Vector Machines}
A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. In two dimensional space, this hyperplane is a line dividing a plane into two parts where in each class lay on either side.\cite{supportvectormachines}
\subsection{Types of SVMs}
There are two kinds of support vector machines: Hard Margin Support Vector Machines and Soft Margin Support Vector Machines.
Hard Margin Support Vector machines do not allow misclassification of data points, tend to overfit the data and are thus sensitive to outliers. Soft margin support vector machines allow slight misclassification of data points and penalize for each misclassification.
For this project, soft margin support vector machines have been used to classify the data points.
\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{supportvectormachines.jpg}
\caption{Support Vector Machine}
\end{figure}
\subsection{Advantages}
The advantages of using support vector machines are:
\begin{itemize}
\item Support vectors can deal with high dimensional data easily with the help of kernel transformations.
\item They are only dependent on some part of the training data known as the support vectors which makes them very memory efficient.
\item Can be used for a variety of tasks due to availability of a large number of kernel functions for decision making.
\item Fast and memory efficient implementations in various languages is available via different libraries.
\end{itemize}
\subsection{Disadvantages}
The disadvantages of using support vector machines are:
\begin{itemize}
\item They do not take the spatial orientation of the pixels into account while learning.
\item With wrong choice of kernel, support vector machines tend to to overfit the data.
\item Hard margin support vector machines are very sensitive to outliers and hence proper care should be taken while using them.
\end{itemize}
\subsection{Project Specifics}
\paragraph{}
After maximizing the separation between classes and minimizing the variance within classes, we have used support vector machines to find hyper-planes separating different classes. Soft margin support vector machines were used for this project. While finding the dividing hyper-plane, soft margin support vector machines allowed some mis-classification with penalty for each mis-classification. 
\paragraph{}
Class weight were balanced for each class. This means that the penalty for mis-classification of an area like river is higher because its presence is lesser. One vs One method was used for multi-class classification therefore for classifying a point all the classes were taken into account- two at a time and then the point was classified to a class getting maximum votes. Linear Kernel was preferred after trying other kernels due to its higher accuracy. For internal calculations of maxima and minima, dual was used of Lagrange's Multipliers were used. The support vector machine was implemented from sci-kit learn library.
\subsection {Decision Trees}
\subsection{Overview}
A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements. Decision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning.
A decision tree is a flowchart-like structure in which each internal node represents a "test" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.

In decision analysis, a decision tree and the closely related influence diagram are used as a visual and analytical decision support tool, where the expected values (or expected utility) of competing alternatives are calculated.

A decision tree consists of three types of nodes:

Decision nodes – typically represented by squares
Chance nodes – typically represented by circles
End nodes – typically represented by triangles
Decision trees are commonly used in operations research and operations management. If, in practice, decisions have to be taken online with no recall under incomplete knowledge, a decision tree should be paralleled by a probability model as a best choice model or online selection model algorithm. Another use of decision trees is as a descriptive means for calculating conditional probabilities.

\subsection{Advantages}
\begin{itemize}
\item Are simple to understand and interpret. People are able to understand decision tree models after a brief explanation.
\item Have value even with little hard data. Important insights can be generated based on experts describing a situation (its alternatives, probabilities, and costs) and their preferences for outcomes.
\item Help determine worst, best and expected values for different scenarios.
\item Use a white box model. If a given result is provided by a model.
\end{itemize}

\subsection{Disadvantages}
\begin{itemize}
\item They are unstable, meaning that a small change in the data can lead to a large change in the structure of the optimal decision tree.
\item They are often relatively inaccurate. Many other predictors perform better with similar data. This can be remedied by replacing a single decision tree with a random forest of decision trees, but a random forest is not as easy to interpret as a single decision tree.
\item For data including categorical variables with different number of levels, information gain in decision trees is biased in favor of those attributes with more levels.
\end{itemize}
\subsection{Project Specifics}
NDVI values have different ranges for different classes, however when analysis is done over a period of time, things become complex. Due to such a range wise  underlying structure it decision trees can make a good model for classification as they also have an if else type structure. The criterion used for choosing the node to split data at every subsequent path is Gini. Gini is calculated as per the given formula:
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{giniformula.png}
\end{figure}
Entropy is another impurity calculation method, however Gini was used as model based on Gini calculations gave higher accuracy.
\subsection{K-Nearest Neighbors}
\subsection{Overview}
The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.
\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{knnone.png}
\caption{K-Nearest Neighbor Visualization}
\end{figure}
Notice in the image above that most of the time, similar data points are close to each other. The KNN algorithm hinges on this assumption being true enough for the algorithm to be useful. KNN captures the idea of similarity (sometimes called distance, proximity, or closeness) with some mathematics we might have learned in our childhood— calculating the distance between points on a graph. There are other ways of calculating distance, and one way might be preferable depending on the problem we are solving. However, the straight-line distance (also called the Euclidean distance) is a popular and familiar choice.

\subsection{Project Specifics}
K-Nearest Neighbors  classifies a point to a class in which maximum of its closest neighbors belong to. Over a period of time, pixels belonging to the same classes tend to have similar features and hence they are closer. Thus, K-Nearest Neighbors can be a very accurate method to classify the pixels. The distance calculated here is the simple Euclidean Distance. Its formula is as follows:
The vote of each class point has uniform weight-age.



\section{U-Net Architecture}
Proposed by \textit{Ronneberger et al.}\cite{unet}, the U-Net architecture builds upon and extends the FCN architecture for more precise segmentation using a smaller training dataset. While the architecture was designed particularly for bio-medical segmentation applications, it has shown to work well for other domains too.
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{unet1.jpg}
\caption{The U-Net architecture.}
\cite{unet}
\end{figure}
\subsection{Components}
The U-Net architecture can be divided into three major parts:
\begin{enumerate}
	\item The down-sampling path
	\item The bottleneck
	\item The up-sampling path
\end{enumerate}
\subsubsection{Down-sampling Path}
It is made up of four identical blocks. Each block encloses the following sequence of layers:
\begin{enumerate}
	\item Convolutional layer ($f=3, s=1, p=0$)
	\item Convolutional layer ($f=3, s=1, p=0$)
	\item Max Pooling layer ($f=2, s=2$)
\end{enumerate}
\paragraph{}
At each down-sampling step (max-pooling), the number of channels is doubled, starting with 64 channels in the first block. The down-sampling path captures not only features relevant to classification, but also contextual information about the location of different segments.
\subsubsection{Bottleneck}
It is made up of 2 layers:
\begin{enumerate}
	\item Convolutional layer ($f=3, s=1, p=0$)
	\item Convolutional layer ($f=3, s=1, p=0$)
%dropout
\end{enumerate}
\subsubsection{Up-sampling Path}
It is made up of four identical blocks. Each block encloses the following sequence of layers:
\begin{enumerate}
	\item Up-sampling transpose convolution layer ($f=2, s=2$)
	\item Concatenation with appropriately cropped feature map from down-sampling path
	\item Convolutional layer ($f=3, s=1, p=0$)
	\item Convolutional layer ($f=3, s=1, p=0$)
\end{enumerate}
\paragraph{}
At each up-sampling step (transpose convolution), the number of channels is halved. The output volume after up-sampling is concatenated with the corresponding volume from the down-sampling path after adequate cropping. The introduction of these skip connections allows the propagation of context information about the localization from the down-sampling path to the up-sampling path which already carries semantic information. This helps the network to spread out activations correctly when up-sampling and lends the network its U-shape.
\subsection{Implementation}
A convolutional neural network based on the above architecture was implemented using TensorFlow in Python. However, some changes were made to enhance the performance of the model.
\begin{enumerate}
\item Convolutional layers down-sample the image, leading to shrinkage of the image's spatial dimensions. This is particularly troublesome for skip-connections as concatenation of volumes at the time of up-sampling requires that the width and height of the volumes match. Using \textbf{same padding} before applying the convolution operation prevents this down-sizing and solves the problem. This also enhances the symmetry in the network, with the output layer having the same spatial dimensions as the input image, reducing the overhead of image processing which would otherwise be required for training the network. 
\item As a neural network is trained, the distribution of each layer's inputs changes continuously, as the parameters of previous layers change. This slows down learning and necessitates careful parameter initialization. This problem of internal covariate shift can be addressed by normalizing layer inputs to a learnable mean and variance, stabilizing the distribution of the activations. \textbf{Batch Normalization} allows us to use higher learning rates and makes the network more robust to the choice of hyper-parameters. Additionally, it also has a regularizing effect and helps prevent overfitting.
\item The shape of image accepted as input by the network was fixed to $256 \times 256$. A patch size of 256 was found to provide sufficient spatial context to the network and fit well with the computational constraints.
\end{enumerate}
The model implemented has 31,051,332 parameters, of which, 31,039,556 are trainable. The arrangement of different layers and shapes of output volumes can be seen in the figure below. A more detailed plot can be found in the Appendix.
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{cnn_implement.jpg}
\end{figure}
\subsection{Training}
\subsubsection{Loss Function}
Since the neural network implemented by us uses softmax activation in the output layer, \textbf{categorical cross entropy} was used as the loss function. This is the natural choice for any multi-class classification problem where the target labels are one-hot encoded.\cite{crossentr} The original paper by \textit{Ronneberger et al.}\cite{unet} proposes the use of a pixel-wise loss weight to achieve better biomedical segmentation. However, this particular scheme was found to be irrelevant for the current project domain. The categorical cross entropy loss is computed as follows:
\begin{displaymath}
CE=-\lg{\frac{\exp{s_p}}{\sum_{j}^{C}\exp{s_j}}}
\end{displaymath}
\subsubsection{Optimizer}
\textbf{Adam} (Adaptive Moment Estimation) is an optimization algorithm designed specifically for deep neural networks that uses adaptive learning rates for each parameter.\cite{adam_web} It computes exponentially weighted averages of gradients and squared gradients. 
\begin{displaymath}
m_t=\beta_1m_{t-1}+(1-\beta_1)g_t
\end{displaymath}
\begin{displaymath}
v_t=\beta_2v_{t-1}+(1-\beta_2)g_t^2
\end{displaymath}
where $m_0$ and $v_0$ are zero-initialized and $g_t$ represents the gradient of the cost function with respect to parameter $\theta$ at step $t$.
These are used for updating the parameters after every step of back-propagation.
\begin{displaymath}
\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{v_t}+\epsilon}m_t
\end{displaymath}
Adam reduces the oscillations in gradients associated with mini-batch gradient descent. The smoother loss curve allows for a larger learning rate and hence, quicker convergence. In limited experiments, Adam was found to produce better results for our network as compared to other optimizers such as stochastic gradient descent.
\subsubsection{Hyper-parameters}
We first trained the network with typical values for hyper-parameters on a small training set. A variance problem was observed as the accuracy for testing set was low. Regularization was not found to improve performance. However, providing the network with more data through augmentation was found to help considerably. Batch normalization makes the network robust to the choice of hyper-parameters and so produced significant performance gains. On the whole, training the network for 70 epochs with a learning rate of 0.005 and default values for the other hyper-parameters was found to produce the best results.
\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{train_history.jpg}
\caption{Curves for loss and accuracy against number of epochs.}
\end{figure}
\subsection{Advantages}
The U-Net architecture presented above has several features which lend it better performance than the FCN architecture.
\begin{itemize}
\item
In the FCN architecture, the single up-sampling layer is added at the end of the CNN and has limited number of channels (equal to the number of class labels). The U-Net architecture makes use of multiple up-sampling layers (each with multiple feature channels) in the up-sampling path, interspersed with blocks of convolutional layers. This provides for the opportunity to add more skip connections and so, to fuse contextual and semantic information better, at the time of up-sampling. 
\item
Concatenation is used for combining information from skip connections, as opposed to the summing operation used in the FCN architecture. This helps to retain more information from both sets of activations, without polluting either.
\item
The U-Net architecture is also particularly well suited to applications where labelled training data is not available in abundance. This makes it more attractive for our project on account of the limited computational resources and time available for training the network. We, hence, propose to use this CNN architecture for the problem of land cover classification.
\end{itemize}
\subsection{Disadvantages}
\begin{itemize}
\item Convolutional neural networks do not take temporal variations directly into account. However, it is hoped that providing the network with training data from different time periods should allow it to capture some of the temporal trends.
\item The performance of a deep neural network greatly depends on the amount of training data available. We propose using techniques like data augmentation and transfer learning so as to get better results even with our small training set.
\end{itemize}

% --------------------------------------

% RESULTS
\chapter{Results}

% --------------------------------------

% CONCLUSION
\chapter{Conclusion}

\paragraph{}
Information of land cover classification is very crucial for applications like natural resource management, wildlife habitat protection, urban expansion, damage delineation, target land detection etc. With developments in technology, conventional method of field survey which is both inaccurate and time consuming has been replaced by remotely sensed imagery analysis which is highly accurate and covers wider range. Indian Space Research Organisation (ISRO) and Indian Institute of Remote Sensing (IIRS), Dehradun play an important role in providing information for these applications. Modern statistical and computing techniques like machine learning and deep learning minimize the human intervention in the task of analysis and classification, thus making the outcome very accurate and the process very agile.
\paragraph{}
The project seeks to compare the applicability of machine learning and deep learning techniques to the problem of land cover classification. Support Vector Machines are traditional machine learning models that provide a method for pixel-wise classification based on learning from the temporal variations in the NDVI time series. On the other hand, Convolutional Neural Networks - a class of deep neural networks, use the spatial variations in the infrared and visible bands of the satellite image for segmentation. The two, hence represent very different approaches for solving the problem. However, the project is still at a very nascent stage, and any comment regarding the comparative performance of these two approaches cannot be made yet.

\section{Future Scope}

% ----------------------------------------

% APPENDIX
\chapter*{Appendix}
\addcontentsline{toc}{chapter}{Appendix} 
\section*{List of dependencies}
\addcontentsline{toc}{section}{List of dependencies} 
\begin{tabular}{p{0.3\textwidth} p{0.7\textwidth}}
\textbf{Package} & \textbf{Version}\\
python & 3.6.9\\
numpy & 1.18.5\\
pandas & 1.0.4\\
matplotlib & 3.2.1\\
pillow & 7.0.0\\
gdal & 2.2.2\\
scikit-image & 0.16.2\\
scikit-learn & 0.22.2.post1\\
tensorflow & 2.2.0\\
\end{tabular}

\section*{Code snippets}
\addcontentsline{toc}{section}{Code snippets} 
\subsection*{Data Preparation}
\subsubsection*{NDVI time series data}
\begin{minted}[breaklines=true]{js}
// Bounds of training and testing locations, marked using GEE Explorer
var agraReg = ee.Geometry.Polygon(
	[[[77.59987614966131, 27.551723242916378],
          [77.59987614966131, 27.092973428952195],
          [78.11760686255194, 27.092973428952195],
          [78.11760686255194, 27.551723242916378]]], null, false);
var ahmeda = ee.Geometry.Polygon(
        [[[72.35056899794004, 23.29854363055665],
          [72.35056899794004, 22.801931268739057],
          [72.81062148817442, 22.801931268739057],
          [72.81062148817442, 23.29854363055665]]], null, false);
// Import dataset, filter by region
var dataset = ee.ImageCollection('LANDSAT/LC08/C01/T1_8DAY_NDVI')
              .filterBounds(agraReg);
// Initialize image as single band, add bands corresponding to other periods
var image = dataset
            .filterDate('2015-01-01', '2015-02-28')
            .median().rename('n1');
image = image.addBands(dataset.filterDate('2015-03-01', '2015-04-30').median().rename('n2'))
             .addBands(dataset.filterDate('2015-05-01', '2015-06-30').median().rename('n3'))
             .addBands(dataset.filterDate('2015-08-01', '2015-10-31').median().rename('n4'))
             .addBands(dataset.filterDate('2015-09-01', '2015-10-31').median().rename('n5'))
             .addBands(dataset.filterDate('2015-11-01', '2015-12-31').median().rename('n6'));
// Visualize image by displaying on map
Map.addLayer(image, {min:0.0, max:1.0}, 'NDVI');
// Export image to Google Drive
Export.image.toDrive({
  image:image,
  description:"NDVI_agra",
  region:agraReg,
  crs:"EPSG:4326",
  scale:30
});
\end{minted}

\subsubsection*{Ground truth land cover data}
\begin{minted}[breaklines=true]{js}
function genClasses(image) {
  // select relevant bands
  var dc = image.select('discrete_classification');
  var tc = image.select('tree-coverfraction');
  var uc = image.select('urban-coverfraction');
  // water, if seasonal coverfraction > 25 or permanent coverfraction > 0
  var water = (image.select('water-seasonal-coverfraction').gt(25))
              .or(image.select('water-permanent-coverfraction').gt(0))
              .rename('water');
  // forest, if tree-coverfraction > 30 and not water
  var forest = tc.gt(30).and(water.not()).rename('forest');
  // builtup land, if urban-coverfraction > 40
  var built = uc.gt(40).and(water.not()).and(forest.not()).rename('urban');
  // agricultural land, if none of the above
  var agro = built.not().and(water.not()).and(forest.not()).rename('crop');
  // create band with labels for every class
  var classes = built.multiply(1)
			.add(agro.multiply(2))
			.add(water.multiply(3))
			.add(forest.multiply(4))
			.rename('classes');
  // add all bands to image
  return image.addBands(built)
              .addBands(agro)
              .addBands(water)
              .addBands(forest)
              .addBands(classes);
}

// Import dataset, filter by region
var lc = ee.ImageCollection("COPERNICUS/Landcover/100m/Proba-V/Global")
           .filterBounds(agraReg)
           .first();
// Generate classes
lc = genClasses(lc);
// Visualize image by displaying on map
var visParams = {min:0, max:4, bands:['classes'], palette:['white', 'red', 'lawngreen', 'blue', 'black']};
Map.addLayer(lc, visParams, "LC");
// Export one hot encoded image to Google Drive
Export.image.toDrive({
  image:lc.select(['built', 'agro', 'water', 'forest']),
  description:"LC_agra",
  region:agraReg,
  crs:"EPSG:4326",
  scale:30
});
// Export labelled image to Google Drive
Export.image.toDrive({
  image:lc.select(['classes']),
  description:"LC_agra_labels",
  region:agraReg,
  crs:"EPSG:4326",
  scale:30
});
\end{minted}

\subsection*{Convolutional Neural Network: U-Net Architecture}
\subsubsection*{Import statements}
\begin{minted}[breaklines=true]{python}
import numpy as np
import gdal
from google.colab import files
from matplotlib import pyplot as plt
from PIL import Image
import skimage.transform as skt
import tensorflow as tf
from tensorflow.keras.layers import (
    Input, 
    Conv2D,
    MaxPool2D,
    BatchNormalization,
    Dropout,
    Conv2DTranspose,
    Cropping2D,
    Concatenate
)
from tensorflow.keras.backend import int_shape
\end{minted}

\subsubsection*{Patch generation and reconstruction}
\begin{minted}[breaklines=true]{python}
def get_patches(img_arr, size=256, stride=256, full_coverage=True):
    # check parameters
    if size % stride != 0:
        raise ValueError("size % stride must be equal to 0.")
    if img_arr.ndim != 3:
        raise ValueError("img_arr must be a 3 dimensional array with shape (height, width, depth).")

    # number of patches to be created
    i_max = (img_arr.shape[0] - size) // stride + 1
    j_max = (img_arr.shape[1] - size) // stride + 1
    # extra patches created to completely cover image
    extra_0 = int((img_arr.shape[0] - size) % stride != 0)
    extra_1 = int((img_arr.shape[1] - size) % stride != 0)
    if(not full_coverage):
        extra_0 = 0
        extra_1 = 0
    patches_list = []

    for i in range(i_max):
        for j in range(j_max):
            patches_list.append(
                img_arr[
                    i * stride : i * stride + size,
                    j * stride : j * stride + size
                ]
            )
        if(extra_1):
            patches_list.append(
                img_arr[
                    i * stride : i * stride + size,
                    img_arr.shape[1] - size : img_arr.shape[1]
                ]
            )
    if(extra_0):
        for j in range(j_max):
            patches_list.append(
                img_arr[
                    img_arr.shape[0] - size : img_arr.shape[0],
                    j * stride : j * stride + size
                ]
            )
        if(extra_1):
            patches_list.append(
                img_arr[
                    img_arr.shape[0] - size : img_arr.shape[0],
                    img_arr.shape[1] - size : img_arr.shape[1]
                ]
            )

    return np.array(patches_list)

def reconstruct_from_patches(img_arr, org_img_size, size, stride, full_coverage=True):
    # check parameters
    if type(org_img_size) is not tuple:
        raise ValueError("org_image_size must be a tuple - (height, width).")

    if img_arr.ndim != 4:
        raise ValueError("img_arr must be a 4 dimensional array of shape (num_patches, size, size, depth).")
    
    # number of patches along each dimension
    num_patches = img_arr.shape[0]
    print(num_patches)
    print(img_arr.shape)
    depth = img_arr.shape[3]
    i_max = (org_img_size[0] - size) // stride + 1
    j_max = (org_img_size[1] - size) // stride + 1
    # extra patches created for full coverage
    extra_0 = int((org_img_size[0] - size) % stride != 0)
    extra_1 = int((org_img_size[1] - size) % stride != 0)
    if(not full_coverage):
        extra_0 = 0
        extra_1 = 0

    # initialize image with zeros
    image = np.zeros(
        (org_img_size[0], org_img_size[1], depth), dtype=img_arr[0].dtype
    )
    patch_no = 0

    for i in range(i_max):
        for j in range(j_max):
            image[
                i * stride : i * stride + size,
                j * stride : j * stride + size,
            ] = img_arr[patch_no, :, :]
            patch_no += 1
        if(extra_1):
            image[
                i * stride : i * stride + size,
                org_img_size[1] - size : org_img_size[1]
            ] = img_arr[patch_no, :, :]
            patch_no += 1
    if(extra_0):
        for j in range(j_max):
            image[
                org_img_size[0] - size : org_img_size[0],
                j * stride : j * stride + size,
            ] = img_arr[patch_no, :, :]
            patch_no += 1
        if(extra_1):
            image[
                org_img_size[0] - size : org_img_size[0],
                org_img_size[1] - size : org_img_size[1]
            ] = img_arr[patch_no, :, :]
            patch_no += 1

    return image
\end{minted}
\subsubsection*{Setting up the model}
\begin{minted}[breaklines=true]{python}
# HYPERPARAMETERS
p = {
  'SIZE' : 256,
  'STRIDE' : 256,
  'FULL_COVER' : True,
  'NUM_BANDS' : 6,
  'NUM_CLASSES' : 4,
  'SAME_PADDING' : True,
  'BATCH_NORM' : True,
  'DROPOUT' : False,
  'DROPOUT_RATE': 0.0,
  'EPOCHS' : 20,
  'BATCH_SIZE' : 8,
  'LEARNING_RATE' : 0.001,
  'BETA_1' : 0.99,
  'BETA_2' : 0.999,
  'EPSILON' : 1e-07
}
def unet(p):
    # Padding
    if(p["SAME_PADDING"]):
        pad = 'same'
    else:
        pad = 'valid'
    # Batch Norm
    batch_norm = p['BATCH_NORM']
    # Dropout
    dropout = p['DROPOUT']
    drop_rate = p['DROPOUT_RATE']
    # Input
    input_shape = (p['SIZE'], p['SIZE'], p['NUM_BANDS'])
    input = Input(shape=input_shape)
    x = input

    # Encoder (Down-sampling path)
    filters = 64
    downLayers = []
    for i in range(4):
        x = Conv2D(filters, (3, 3), strides=(1, 1), padding=pad, activation='relu', use_bias=not batch_norm)(x)
        if(batch_norm):
            x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)
        x = Conv2D(filters, (3, 3), strides=(1, 1), padding=pad, activation='relu', use_bias=not batch_norm)(x)
        if(batch_norm):
            x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)
        downLayers.append(x)
        x = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding=pad)(x)
        filters *= 2

    # Bottleneck
    x = Conv2D(filters, (3, 3), strides=(1, 1), padding=pad, activation='relu', use_bias=not batch_norm)(x)
    if(batch_norm):
        x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)
    if(dropout):
        x = Dropout(rate=drop_rate)(x)
    x = Conv2D(filters, (3, 3), strides=(1, 1), padding=pad, activation='relu', use_bias=not batch_norm)(x)
    if(batch_norm):
        x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)
    if(dropout):
        x = Dropout(rate=drop_rate)(x)
    downLayers.reverse()

    # Decoder (Up-sampling path)
    for dlayer in downLayers:
        filters /= 2
        x = Conv2DTranspose(filters, (2, 2), strides=(2, 2), padding=pad)(x)
        # Skip Connection
        cropDim = calcCropDims(int_shape(x), int_shape(dlayer))
        if(not p['SAME_PADDING']):
            crop = Cropping2D(cropDim)(dlayer)
        else:
            crop = dlayer
        x = Concatenate(axis=-1)([x, crop])
        x = Conv2D(filters, (3, 3), strides=(1, 1), padding=pad, activation='relu', use_bias=not batch_norm)(x)
        if(batch_norm):
            x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)
        x = Conv2D(filters, (3, 3), strides=(1, 1), padding=pad, activation='relu', use_bias=not batch_norm)(x)
        if(batch_norm):
            x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)
    # Output
    output = Conv2D(p['NUM_CLASSES'], (1, 1), strides=(1, 1), padding=pad, activation='softmax')(x)

    model = tf.keras.Model(input, output, name='unet')

    return model

# Create instance of model
model = unet(p)

# Compile with choice of optimizer, loss function and metrics for evaluation
model.compile(loss=tf.keras.losses.categorical_crossentropy,
	optimizer=tf.keras.optimizers.Adam(
		learning_rate=p['LEARNING_RATE'], 
		beta_1=p['BETA_1'], 
		beta_2=p['BETA_2'], 
		epsilon=p['EPSILON']
	),
	metrics=[tf.keras.metrics.CategoricalAccuracy()]
)
\end{minted}

\subsubsection*{Loading datasets}
\begin{minted}[breaklines=true]{python}
# Training data
train_x = gdal.Open('/content/drive/My Drive/PS/data/earth_ndvi_6band/NDVI_v2.tif')
train_x = np.array(train_x.ReadAsArray())
train_x = np.moveaxis(train_x, 0, -1)
train_shape = train_x.shape[:2]
train_x_img = train_x
train_x = get_patches(train_x, p['SIZE'], p['STRIDE'], p['FULL_COVER'])
train_y = gdal.Open('/content/drive/My Drive/PS/data/earth_landcover/LC_final_v5.tif')
train_y = np.array(train_y.ReadAsArray())
train_y = np.moveaxis(train_y, 0, -1)
assert(train_y.shape[:2] == train_shape)
train_y_img = train_y
train_y = get_patches(train_y, p['SIZE'], p['STRIDE'], p['FULL_COVER'])
\end{minted}
\subsubsection*{Data Augmentation}
\begin{minted}[breaklines=true]{python}
train_x_aug = []
train_y_aug = []
for i in range(train_x.shape[0]):
    train_x_aug.append(train_x[i])
    train_y_aug.append(train_y[i])
    img_x = train_x[i]
    img_y = train_y[i]
    for j in range(3):
        img_x = skt.rotate(img_x, 90)
        train_x_aug.append(img_x)
        img_y = skt.rotate(img_y, 90)
        train_y_aug.append(img_y)
train_x_aug = np.array(train_x_aug)
train_y_aug = np.array(train_y_aug)
\end{minted}
\subsubsection*{Training the model}
\begin{minted}[breaklines=true]{python}
# Train model and record history
model_history = model.fit(train_x_aug, train_y_aug, batch_size=p['BATCH_SIZE'], epochs=p['EPOCHS'])

def plotTrainingHistory(hist, EPOCHS):
    loss = hist['loss']
    cat_acc = hist['categorical_accuracy']
    epochs = range(EPOCHS)
    plt.figure()
    plt.plot(epochs, loss, 'r', label='Loss')
    plt.plot(epochs, cat_acc, 'b', label='Accuracy')
    plt.legend()
    plt.show()

# Plot loss curve and accuracy against epochs
plotTrainingHistory(model_history.history, p['EPOCHS'])
\end{minted}

\subsubsection*{Evaluting the model}
\begin{minted}[breaklines=true]{python}
def oneHotFromActivations(testset):
    """
    For vector at each pixel, sets element corresponding to predicted class to 1, rest to 0.
    """
    for t in range(testset.shape[0]):
        for h in range(testset.shape[1]):
            for w in range(testset.shape[2]):
                maxcell = np.max(testset[t][h][w])
                for cell in range(testset.shape[3]):
                    if(testset[t][h][w][cell] == maxcell):
                        testset[t][h][w][cell] = 1
                    else:
                        testset[t][h][w][cell] = 0
    return testset

def viewPredictions(pred, img_shape, stride, size):
    """
    Takes output of model.predict().
    One hot encodes the predictions, restitches patches to get original image, returns a RGB visual representation of predictions.
    """
    pred = oneHotFromActivations(pred)
    pred = np.int32(pred) * 255
    pred = np.moveaxis(pred, -1, 0)[:3]
    pred = np.moveaxis(pred, 0, -1)
    return reconstruct_from_patches(pred, img_shape, stride, size)

def iou(preds, labels):
    """
    Calculates Intersection over Union for predictions, comparing with ground truth data.
    Returns a dict containing class-wise IoUs, mean IoU, and categorical accuracy.
    """
    h, w = preds.shape[1:3]
    num_classes = preds.shape[0]

    intersection = []
    union = []

    for c in range(num_classes):
        intersection_c = np.sum((np.logical_and(preds[c] == 1, labels[c] == 1)))
        intersection.append(intersection_c)
    for c in range(num_classes):
        union_c = np.sum((np.logical_or(preds[c] == 1, labels[c] == 1)))
        union.append(union_c)

    intersection = np.array(intersection)
    union = np.array(union)
    return (intersection / union, intersection.sum() / (h * w))

def testPredictions(preds, labels, img_shape, size, stride):
    """
    preds as output by model.predict.
    labels as output by gdal, cast to a numpy array.
    img_shape must be a tuple - (height, width).
    Evaluates the predictions using accuracy and IoU.
    """
    pred_img = oneHotFromActivations(preds)
    pred_img = reconstruct_from_patches(pred_img, img_shape, size, stride)
    pred_img = np.moveaxis(pred_img, -1, 0)
    ious, accuracy = iou(pred_img, labels)
    return {
        'class_ious': ious,
        'mean_iou': ious.mean(),
        'accuracy': accuracy 
    }

# Visualize predictions
test_pred = model.predict(test_x)
test_pred_img = viewPredictions(test_pred, test_shape, p['SIZE'], p['STRIDE'])
plt.imshow(test_pred_img)

# Evaluate predictions
test_y = gdal.Open('/content/drive/My Drive/PS/data/earth_landcover/LC_ahmeda.tif')
test_y = np.array(test_y.ReadAsArray())
print(testPredictions(test_pred, test_y, test_shape, p['SIZE'], p['STRIDE']))
\end{minted}

\subsubsection*{Sample for code}
\begin{minted}[breaklines=true]{python}
# CODE GOES HERE, PLS COPY AND LEAVE HERE
\end{minted}

% ---------------------------------------

% REFERENCES
\printbibliography[heading=bibintoc, title={References}]

\setcounter{secnumdepth}{0}
\section*{List of Abbreviations}
\addcontentsline{toc}{chapter}{List of Abbreviations}  
\vspace*{0.5cm}
\begin{tabular}{p{0.2\textwidth} p{0.8\textwidth}}
\textbf{CNN} & Convolutional Neural Networks\\
\textbf{ET} & Evapotranspiration\\
\textbf{FCN} & Fully Convolutional Neural Networks\\
\textbf{GEE} & Google Earth Engine\\
\textbf{GPU} & Graphical Processing Unit\\
\textbf{IIRS} & Indian Intitute of Remote Sensing\\
\textbf{NDVI} & Normalised Difference Vegetation Index\\
\textbf{NIR} & Near InfraRed\\
\textbf{NN} & Neural Networks.\\
\textbf{SVM} & Support Vector Machines\\
\end{tabular}
\vspace*{1cm}

% GLOSSARY
\section*{Glossary}
\addcontentsline{toc}{chapter}{Glossary}  
\vspace*{0.5cm}
\textbf{Chlorophyll} green photosynthetic pigment found in plants\\
\textbf{Convolution} mathematical way of combining two signals to form a third signal\\
\textbf{Epoch} one complete pass through the entire training set\\
\textbf{Kernel} Kernel functions.\\
\textbf{Pixel} a minute area of illumination on a display screen, one of many from which an image is composed.\\
\textbf{Pooling} process of extracting features from image output of a convolution layer\\
\textbf{Semantic Segmentation} the process of associating each pixel of an image with a class label\\
\textbf{Temporal} relating to time\\


\end{document}
